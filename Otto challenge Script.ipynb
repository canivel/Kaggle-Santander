{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Otto Group Product Classification Problem\n",
    "\n",
    "This is a challenge from Kaggle challenges.\n",
    "In this problem we want to classifiy Otto's products into 9 categories, each product is described with some annoynomes features. \n",
    "you can find the challenge here :  \n",
    "https://www.kaggle.com/c/otto-group-product-classification-challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "we have about 62000 samples for training and 144000 samples for testing.\n",
    "\n",
    "Files:  \n",
    "trainData.csv - the training set  \n",
    "testData.csv - the test set\n",
    "\n",
    "Data fields:  \n",
    "id - an anonymous id unique to a product  \n",
    "feat_1, feat_2, ..., feat_93 - the various features of a product  \n",
    "target - the class of a product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Evaluation\n",
    "\n",
    "Submissions are evaluated using the multi-class logarithmic loss.  \n",
    "\n",
    "<img src=\"log-loss.png\">\n",
    "\n",
    "where N is the number of products in the test set, M is the number of class labels, log is the natural logarithm, yij is 1 if observation i is in class j and 0 otherwise, and pij is the predicted probability that observation i belongs to class j."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution\n",
    "\n",
    "First these are the libraries we will use : Pandas, NumPy, SciPy,Scikit-learn and XGBoost.  \n",
    "Thanks for Anaconda (python package manager) all needes libraries are included but XGBoost.  \n",
    "To install XGBoost : https://xgboost.readthedocs.org/en/latest/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import ensemble\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import grid_search\n",
    "from sklearn import cross_validation;\n",
    "import xgboost as xgb\n",
    "from sklearn import neighbors\n",
    "from sklearn import cluster\n",
    "from sklearn.metrics import pairwise\n",
    "from sklearn.externals import joblib\n",
    "from sklearn import feature_selection\n",
    "from sklearn import decomposition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading training and testing data\n",
    "Using pandas library we can read the train file into a Dataframe called trainData.  \n",
    "then separate the trainData into 2 Dataframes one with the features and one with the correct output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainData= pd.read_csv(\"train.csv\");\n",
    "correctClass=trainData['target'];\n",
    "classes=np.unique(correctClass);\n",
    "trainFeatures=trainData.drop(['id', 'target'], axis=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testData= pd.read_csv(\"test.csv\");\n",
    "testFeatures=testData.drop(['id'], axis=1);\n",
    "testIDs=testData['id'];\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print some information about the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            feat_1        feat_2        feat_3        feat_4        feat_5  \\\n",
      "count  61878.00000  61878.000000  61878.000000  61878.000000  61878.000000   \n",
      "mean       0.38668      0.263066      0.901467      0.779081      0.071043   \n",
      "std        1.52533      1.252073      2.934818      2.788005      0.438902   \n",
      "min        0.00000      0.000000      0.000000      0.000000      0.000000   \n",
      "25%        0.00000      0.000000      0.000000      0.000000      0.000000   \n",
      "50%        0.00000      0.000000      0.000000      0.000000      0.000000   \n",
      "75%        0.00000      0.000000      0.000000      0.000000      0.000000   \n",
      "max       61.00000     51.000000     64.000000     70.000000     19.000000   \n",
      "\n",
      "             feat_6        feat_7        feat_8        feat_9       feat_10  \\\n",
      "count  61878.000000  61878.000000  61878.000000  61878.000000  61878.000000   \n",
      "mean       0.025696      0.193704      0.662433      1.011296      0.263906   \n",
      "std        0.215333      1.030102      2.255770      3.474822      1.083340   \n",
      "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "75%        0.000000      0.000000      1.000000      0.000000      0.000000   \n",
      "max       10.000000     38.000000     76.000000     43.000000     30.000000   \n",
      "\n",
      "           ...            feat_84       feat_85       feat_86       feat_87  \\\n",
      "count      ...       61878.000000  61878.000000  61878.000000  61878.000000   \n",
      "mean       ...           0.070752      0.532306      1.128576      0.393549   \n",
      "std        ...           1.151460      1.900438      2.681554      1.575455   \n",
      "min        ...           0.000000      0.000000      0.000000      0.000000   \n",
      "25%        ...           0.000000      0.000000      0.000000      0.000000   \n",
      "50%        ...           0.000000      0.000000      0.000000      0.000000   \n",
      "75%        ...           0.000000      0.000000      1.000000      0.000000   \n",
      "max        ...          76.000000     55.000000     65.000000     67.000000   \n",
      "\n",
      "            feat_88       feat_89       feat_90       feat_91       feat_92  \\\n",
      "count  61878.000000  61878.000000  61878.000000  61878.000000  61878.000000   \n",
      "mean       0.874915      0.457772      0.812421      0.264941      0.380119   \n",
      "std        2.115466      1.527385      4.597804      2.045646      0.982385   \n",
      "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "75%        1.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "max       30.000000     61.000000    130.000000     52.000000     19.000000   \n",
      "\n",
      "            feat_93  \n",
      "count  61878.000000  \n",
      "mean       0.126135  \n",
      "std        1.201720  \n",
      "min        0.000000  \n",
      "25%        0.000000  \n",
      "50%        0.000000  \n",
      "75%        0.000000  \n",
      "max       87.000000  \n",
      "\n",
      "[8 rows x 93 columns]\n"
     ]
    }
   ],
   "source": [
    "print(trainFeatures.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that there is no missing values in the training/testing features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(sum(trainFeatures.count(axis=1)< 93) )\n",
    "print(sum(testFeatures.count(axis=1) < 93) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Using XGBoost classifier\n",
    "\n",
    "we will use the sklearn wrapper of XGBoost (installed with XGBoost).  \n",
    "- set the objective parmater to 'multi:softprob' becuase our problem is multiclass classification and we want to output a probability for each class.  \n",
    "- use cross validation with kfolds=4.  \n",
    "\n",
    "First try without paramters tunning.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "XGBClassifier=xgb.XGBClassifier(objective='multi:softprob');\n",
    "crossValidator= cross_validation.StratifiedKFold(correctClass,n_folds=4);    \n",
    "scores=cross_validation.cross_val_score(classifier,trainFeatures,verbose=10,y=correctClass,cv=crossValidator,scoring='log_loss');\n",
    "print('crossValidation error = ',scores.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "crossValidation error = 0.656282936478"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paramters Tunning using Grid Search\n",
    "use gird search to tune the paramters accorading to the best 'log_loss'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "XGBClassifier=xgb.XGBClassifier(objective='multi:softprob');\n",
    "parameters = {\n",
    "        'learning_rate': [0.1,0.3],\n",
    "        'n_estimators': [200,400,600], \n",
    "        'max_depth': [9,12],\n",
    "        'gamma' :[1],       \n",
    "        'subsample': [0.5],\n",
    "        'colsample_bytree': [1],\n",
    "        }\n",
    "\n",
    "classifier=grid_search.GridSearchCV(XGBClassifier,parameters,scoring='log_loss',cv=4);                               \n",
    "classifier.fit(trainFeatures,correctClass);\n",
    "\n",
    "best_parameters, score, _ = max(classifier.grid_scores_, key=lambda x: x[1])\n",
    "print('best cv score is ',score)\n",
    "print('best paramters are')\n",
    "for param_name in sorted(best_parameters.keys()):\n",
    "     print(\"%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best cv score is  0.466627075491  \n",
    "best paramters are  \n",
    "colsample_bytree: 1  \n",
    "gamma: 1  \n",
    "learning_rate: 0.1  \n",
    "max_depth: 9  \n",
    "n_estimators: 400  \n",
    "subsample: 0.5  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding new features\n",
    "\n",
    "feature 1: number of non zeros in the feature vector.  \n",
    "feature 2: sum of all features.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sumColumn=trainFeatures.apply(lambda row: row.sum(), axis=1);\n",
    "nonZerosColumn=trainFeatures.apply(lambda row: sum(row!=0), axis=1);\n",
    "trainFeatures['sum']=sumColumn\n",
    "trainFeatures['nonZero']=nonZerosColumn;\n",
    "\n",
    "sumColumnT=testFeatures.apply(lambda row: row.sum(), axis=1);\n",
    "nonZerosColumnT=testFeatures.apply(lambda row: sum(row!=0), axis=1);\n",
    "testFeatures['sum']=sumColumnT\n",
    "testFeatures['nonZero']=nonZerosColumnT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search again with the new features and get :  \n",
    "        \n",
    "best cv score is  0.459499757896  \n",
    "best paramters are  \n",
    "colsample_bytree: 0.8  \n",
    "colsample_bylevel: 0.8  \n",
    "min_child_weight=0.8  \n",
    "gamma: 1  \n",
    "learning_rate: 0.1  \n",
    "max_depth: 9  \n",
    "n_estimators: 400  \n",
    "subsample: 0.8  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model\n",
    "\n",
    "The model now is performing good, next step is train two models with different seeds and best paramters then average the two models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "XGBClassifier1 =xgb.XGBClassifier(objective='multi:softprob',n_estimators=400,max_depth=9,learning_rate=0.1,subsample=0.8\n",
    "                                  ,colsample_bytree=0.8,colsample_bylevel=0.8,min_child_weight=0.8,gamma=1,seed=23);\n",
    "XGBClassifier2 =xgb.XGBClassifier(objective='multi:softprob',n_estimators=400,max_depth=9,learning_rate=0.1,subsample=0.8\n",
    "                                  ,colsample_bytree=0.8,colsample_bylevel=0.8,min_child_weight=0.8,gamma=1,seed=100);\n",
    "\n",
    "classifier=ensemble.VotingClassifier([('clf1',XGBClassifier1),('clf2',XGBClassifier2)],voting='soft',weights=[1,1]);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "these model result in cross validation error = 0.459256230407 which is better than the previous one.  \n",
    "let's train these model on the full training data and make a submission on kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier.fit(trainFeatures,correctClass);          \n",
    "testingPreds=classifier.predict_proba(testFeatures);\n",
    "submission = pd.DataFrame(data=testingPreds,columns=classes)\n",
    "submission.insert(0,'id',testIDs)\n",
    "submission.to_csv(\"XGBoost.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Kaggle score = 0.43902 and rank 478."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
